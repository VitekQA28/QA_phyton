import requests
from bs4 import BeautifulSoup
import urllib.robotparser
import xml.etree.ElementTree as ET

base_url = 'https://prostayaeda.s2.citruspro.ru/'

pages = [
    {'name': '', 'label': 'Главная'},
    {'name': 'catalogs', 'label': 'Меню'}, 
    {'name': 'delivery', 'label': 'Условия доставки'}, 
    {'name': 'stocks', 'label': 'Акции'}, 
    {'name': 'about', 'label': 'О нас'}, 
    {'name': 'contacts', 'label': 'Наши адреса'},
    {'name': 'vacancies', 'label': 'Вакансии'}, 
    {'name': 'legal-info', 'label': 'Правовая информация'}, 
    {'name': 'personals', 'label': 'Личный кабинет'}, 
    {'name': 'orders', 'label': 'Заказы'}
]


class Pr_eda:
    def __init__(self):
        self.index = urllib.robotparser.RobotFileParser()
        self.index.set_url(base_url + "robots.txt")
        self.index.read()

    def test_robots(self):
        assert self.index.can_fetch("*", base_url), 'Сайт закрыт от индексации'       
    
    
    def test_status_page(self):                
        for page in pages:
            resp = requests.get(base_url + page['name'])
            try:
                assert resp.status_code == 200
            except AssertionError as error:                      
                error_message = True
                print(f"Страница {page['label']} недоступна. Код статуса: {resp.status_code}")      
        if error_message:
            assert False, "Страницы недоступны:\n"       
                               


    def test_meta_tags(self):        
        error_pages = []
        for page in pages:
            resp = requests.get(base_url + page['name'])            
            soup = BeautifulSoup(resp.content, "html.parser")
            attributes = ["title", "keywords", "description"]
            for attribute in attributes:
                try:
                    element = soup.find("meta", {"name": attribute})
                    assert element is not None, f"Нет элемент <meta> с атрибутом name='{attribute}' на странице {page['label']}"
                    content = element.get("content")
                    assert content != "", f"Атрибут 'content' элемента <meta> с атрибутом name='{attribute}' пустой на странице {page['label']}"
                except AssertionError as error:
                    error_pages.append(f"На странице {page['label']} нет элемента <meta> с атрибутом name='{attribute}'")
        assert not error_pages, f"\n".join(error_pages)
           

    def test_footer_element(self):
        error_pages = []
        for page in pages:
            resp = requests.get(base_url + page['name'])            
            soup = BeautifulSoup(resp.content, "html.parser")
            try:
                element = soup.find("footer", class_="b-d") #"b-f"
                assert element is not None, f"Нет элемента <footer> на странице {page['label']}"
            except AssertionError as error:
                error_pages.append(f"На странице {page['label']} не удалось найти элемент <footer>")
        assert not error_pages, "\n".join(error_pages)


    def test_sitemap(self):
        resp = requests.get(base_url + 'sitemap.xml')
        assert resp.status_code == 200, "На сайте отсутствует sitemap"
        xml_content = resp.content
        try:
            root = ET.fromstring(xml_content)
            assert len(root) > 0, "Sitemap пустой"
            print("Sitemap не пустой")
        except ET.ParseError:      
            assert False, "Некорректный формат XML"